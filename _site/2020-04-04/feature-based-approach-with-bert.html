<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Feature-based Approach with BERT &middot; Trishala
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
</head>


  <body>
    <nav class="nav">
      <div class="nav-container">
        <a href="/">
          <h2 class="nav-title">Trishala</h2>
        </a>
        <ul>
          <li><a href="/about">About</a></li>
          <li><a href="/">Posts</a></li>
        </ul>
    </div>
  </nav>

    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Trishala Neeraj
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2020-04-04 00:00:00 -0700">April 04, 2020</time>
    
  </div>

  <h1 class="post-title">Feature-based Approach with BERT</h1>
  <div class="post-line"></div>

  <p>BERT is a language representation model pre-trained on a very large amount of unlabeled text corpus over different pre-training tasks. It was proposed in the paper <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al., 2018). BERT is deeply bidirectional, i.e., it pre-trains deep bidirectional representations from text by jointly conditioning on context in both directions. In other words, to represent a word in a sentence, BERT will use both its previous as well as its next context in contrast to:</p>
<ol>
  <li>context-free models like <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">word2vec</a> (Mikolov et al., 2013) or <a href="https://nlp.stanford.edu/pubs/glove.pdf">Glove</a> (Pennington et al., 2014)</li>
  <li>shallowly bidirectional contextual models like <a href="https://arxiv.org/abs/1802.05365">ELMo</a> (Peters et al., 2018)</li>
  <li>unidirectional contextual models like <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI GPT</a>(Radford et al., 2018).</li>
</ol>

<p>BERT can be fine-tuned on a variety of downstream NLP tasks (such as entailment, classification, tagging, paraphrasing and question answering) without a lot of task-specific modifications to the architecture. Compared to pre-training, fine-tuning is relatively inexpensive and is able to achieve state-of-the-art performance on various sentence-level as well as token-level tasks. The authors have released the <a href="https://github.com/google-research/bert">open-source TensorFlow</a> implementation.</p>

<p>The paper by Devlin et al. also discusses the advantages of a feature-based approach to directly utilizing the features extracted from the pre-trained model. The authors compare the two approaches (fine-tuning vs feature-based) by applying BERT to the <a href="https://arxiv.org/abs/cs/0306050">CoNLL-2003 Named Entity Recognition (NER) task</a> (Tjong Kim Sang and De Meulder, 2003). The feature-based approach here comprised of extracting the activations (or contextual embeddings or token representations or features) from one or more of the 12 layers without fine-tuning any parameters of BERT. These embeddings are then used as input to a BiLSTM followed by the classification layer for NER. The authors report that when they concatenate the token representations from the top four hidden layers of the pre-trained Transformer and use that directly in the downstream task, the performance achieved is comparable to fine-tuning the entire model (including the parameters of BERT).</p>

<p>I am fascinated by this result and sought to replicate an experimental setup similar to this work. In this blog post, I will extract and study contextual embeddings from the first sub-token (AKA [CLS] token) at each layer. I will detail my work on a text classification task using a similar feature-based approach that achieves comparable results when utilizing embeddings from any of the 12 Transformer layers in BERT.</p>

<h1 id="data">Data</h1>

<p>In this blog post, I will work through a text classification task. The dataset I’ve chosen is the <a href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification">Jigsaw Multilingual Toxic Comment Classification</a> dataset. The training data is English-only and comprises of the text of comments made in online conversations as well as a boolean field specifying if a given comment has been classified as toxic or non-toxic. The task is to predict if a comment is toxic. Here’s a sample of what the data looks like -</p>

<table>
  <thead>
    <tr>
      <th>sentence</th>
      <th style="text-align: center">label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Hey man, I’m really not trying to edit war. It’s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td>Worse case scenario we do add it with overview (make it an extended history/misison/overview) Banner has been added to talkpage and subheadings to the SandBox page start posting info and change headings if needed but be sure to indicate so in the edit summary bar</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<p>I’d be working with a small sample of 1000 comments to get started.</p>

<h1 id="tokenization">Tokenization</h1>

<p>BERT’s model architecture is a multi-layer bidirectional Transformer encoder. In this blog, I’d be working with the BERT “base” model which has 12 Transformer blocks or layers, 16 self-attention heads, hidden size of 768.</p>

<p>Input data needs to be prepared in a special way. BERT uses WordPiece embeddings (Wu et al.,2016) with a 30,000 token vocabulary. There are 2 special tokens that are introduced in the text –</p>

<ul>
  <li>a token [SEP] to separate two senteces, and</li>
  <li>a classification token [CLS] which is the first token of every tokenized sequence.</li>
</ul>

<p>The authors state that the final hidden state corresponding to the [CLS] token is used as the aggregate sequence representation for classification tasks. I am interested in looking into how meaningful these representations are across each of the 12 layers. Here’s how to obtain these embeddings from the dataset I’ve selected.</p>

<h3 id="tokenize-each-sentence-ie-split-each-sentence-into-tokens">Tokenize each sentence, i.e., split each sentence into tokens</h3>

<p>For example, consider the following non-toxic comment from the dataset:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Please stop vandalising wikipedia immediately or you will be blocked
</code></pre></div></div>

<p>This sentence upon tokenization would look like this:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'please', 'stop', 'van', '##dal', '##ising', 'wikipedia', 'immediately', 'or', 'you', 'will', 'be', 'blocked'
</code></pre></div></div>

<p>Notice, the tokens are either the words lowercases, or a leading-subword from a larger word or a trailing-subword from a larger word (marked with a <code class="highlighter-rouge">##</code> to indicate so).</p>

<h3 id="add-special-tokens--cls-in-the-beginning--sep-as-a-separator-between-sentences-or-at-the-end-of-a-single-sentence">Add special tokens – [CLS] in the beginning &amp; [SEP] as a separator between sentences (or at the end of a single sentence)</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[CLS], 'please', 'stop', 'van', '##dal', '##ising', 'wikipedia', 'immediately', 'or', 'you', 'will', 'be', 'blocked', [SEP]
</code></pre></div></div>

<h3 id="pad-tokenized-sequences-to-the-maximum-length-or-truncate-sequences-to-a-fixed-size">Pad tokenized sequences to the maximum length (or truncate sequences to a fixed size)</h3>

<p>For this dataset, I’ve chosen the maximum length of 64. Sequences with lesser than 64 tokens will be padded to meet this length, and sequences with more would be truncated.</p>

<h3 id="convert-tokens-to-ids-and-convert-to-tensors">Convert tokens to IDs and convert to tensors</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([  101,  3531,  2644,  3158,  9305,  9355, 16948,  3202,  2030,  2017,
         2097,  2022,  8534,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0])
</code></pre></div></div>

<h3 id="create-an-attention-masks-also-tensors-to-explicitly-identify-tokens-that-are-actually-pad-tokens">Create an attention masks (also tensors) to explicitly identify tokens that are actually PAD tokens.</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</code></pre></div></div>

<p>I utilized the <code class="highlighter-rouge">BertTokenizer</code> made available as part of the <a href="https://huggingface.co/transformers/">Hugging Face Transformer library</a> to perform the above operations. The code snippet below gets us the following 2 tensors to feed into the model later -</p>
<ul>
  <li>input_ids</li>
  <li>attention_masks</li>
</ul>

<p>They should both have the same shape of (num_samples x max_len), in the case of this work – (1000 x 64).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

input_ids = []
attention_masks = []
for sentence in df['sentence'].tolist():
    dictionary = tokenizer.encode_plus(
                        sentence,                      
                        add_special_tokens = True,
                        max_length = 64,
                        pad_to_max_length = True,
                        return_attention_mask = True,
                        return_tensors = 'pt',
                   )
    # encode_plus returns a dictionary 
    input_ids.append(dictionary['input_ids'])
    attention_masks.append(dictionary['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
</code></pre></div></div>

<h1 id="contextual-embeddings">Contextual Embeddings</h1>

<p>At this point, I have the input IDs and attention masks for the text data, and the next step is to extract and study the embeddings. For this, I used the <code class="highlighter-rouge">bert-base-uncased</code> pre-trained model made available by Hugging Face transformers. As I mentioned in the above section, it is trained on lower-cased English (12-layer, 768-hidden, 12-heads, 110M parameters). When loading the pre-trained model, make sure to set <code class="highlighter-rouge">output_hidden_states</code> parameter True which will give us access to output embeddings from all 12 Transformer layers.</p>

<p>The snippet below shows that the model takes as input <code class="highlighter-rouge">input_ids</code> – the indices obtained in the previous section, as well as attention masks.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="p">=</span> <span class="n">BertConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-uncased"</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="p">=</span><span class="nb">True</span><span class="p">)</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-uncased"</span><span class="p">,</span> <span class="n">config</span><span class="p">=</span><span class="n">config</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="p">=</span> <span class="k">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">=</span><span class="n">attention_masks</span><span class="p">)</span>
</code></pre></div></div>

<p>The tuple <code class="highlighter-rouge">outputs</code> comprises of the following tensors(based on <a href="https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel">documentation</a> and <a href="https://github.com/huggingface/transformers/issues/1827">this GitHub Issue</a>):</p>
<ul>
  <li>last_hidden_state</li>
  <li>pooler_output</li>
  <li>hidden_states</li>
</ul>

<p>In this work, I’m most interested in the <code class="highlighter-rouge">hidden_states</code> which is a tuple of 3 tensors. The last element of this tuple contains the contextual embeddings that each Transformer layer outputs. They can be accessed like this -</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>embeddings = outputs[2][1:]
</code></pre></div></div>

<p>Here, <code class="highlighter-rouge">embeddings</code> is another tuple of size 12, containing 12 sets of contextual embeddings – one from each layer. Note that these embeddings represent all tokens in the input sequence padded / truncated to the maximum fixed length. Each tensor in <code class="highlighter-rouge">embeddings</code> is of the shape – (num_samples x max_length x hidden_size), in this case (1000 x 64 x 768).</p>

<p>In this work, I’m focussed on the downstream task of text classification, and therefore would only work with representations encoding [CLS] tokens, because as mentioned in the previous section, they are the aggregate sequence representation for classification tasks. Therefore, I will slice the embeddings -</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_CLS_embedding(layer):
    return layer[:, 0, :].numpy()
    
cls_embeddings = []
for i in range(12):
    cls_embeddings.append(get_CLS_embedding(embeddings[i]))
</code></pre></div></div>
<p>Each of these 12 NumPy arrays in the list <code class="highlighter-rouge">cls_embeddings</code> would be of shape (num_samples x hidden_size), in this case (1000 x 768), and each of these can be used as train a text classifier. As a reminder, I will not be fine-tuning, but simply using these features extracted from BERT as input features to my model.</p>

<h1 id="performance-on-text-classification">Performance on Text Classification</h1>

<p>I trained a Logistic Regression model using these <code class="highlighter-rouge">cls_embeddings</code> as features, one layer at a time. Below is the accuracy for each layer on a fixed validation set.</p>

<table>
  <thead>
    <tr>
      <th>cls_embedding from layer #</th>
      <th style="text-align: center">accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td style="text-align: center">0.812</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: center">0.86</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: center">0.856</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: center">0.868</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: center">0.868</td>
    </tr>
    <tr>
      <td>6</td>
      <td style="text-align: center">0.844</td>
    </tr>
    <tr>
      <td>7</td>
      <td style="text-align: center">0.848</td>
    </tr>
    <tr>
      <td>8</td>
      <td style="text-align: center">0.804</td>
    </tr>
    <tr>
      <td>9</td>
      <td style="text-align: center">0.824</td>
    </tr>
    <tr>
      <td>10</td>
      <td style="text-align: center">0.844</td>
    </tr>
    <tr>
      <td>11</td>
      <td style="text-align: center">0.828</td>
    </tr>
    <tr>
      <td>12</td>
      <td style="text-align: center">0.856</td>
    </tr>
  </tbody>
</table>

<p>From the results, it looks like more than one of the 12 layers helps achieve good accuracy without any fine-tuning, and using a basic logistic regression model.</p>

<h1 id="future-work">Future Work</h1>

<p>In future work, I will fine-tune on this task and explore tradeoffs to using it over the fine-tuning approach. While the feature-based approach might be a good and economical option for a large number of use-cases, fine-tuning should boost performance further.</p>


</div>

<div class="pagination">
  
  
    <a href="/2017-12-22/semantic-entailment" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>

    <footer>
      <span>
        &copy; <time datetime="2020-04-11 18:18:10 -0700">2020</time> Trishala Neeraj. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme.
      </span>
    </footer>
  </body>
</html>
